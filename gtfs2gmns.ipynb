{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asu-trans-ai-lab/GTFS2GMNS/blob/main/gtfs2gmns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparation"
      ],
      "metadata": {
        "id": "7cCSGNLJf3z7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ghEaf-d2Fdm"
      },
      "source": [
        "**Load the GTFS file from the repository of GTFS testing datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3xZMKsJ2Ew2",
        "outputId": "7d981fdb-2670-48df-e919-2e6534a4f6b2"
      },
      "source": [
        "!rm -rf ./GTFS2GMNS/\n",
        "!git clone https://github.com/FangTang999/GTFS2GMNS\n",
        "\n",
        "%cd GTFS2GMNS/test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GTFS2GMNS'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 72 (delta 0), reused 0 (delta 0), pack-reused 67\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n",
            "/content/GTFS2GMNS/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CexwTGDB0D0A"
      },
      "source": [
        "Check the file icon on the left hand side, makesure files stop.txt, route.txt, trip.txt, stop_times.txt, agency.txt exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6y7B8WX-d46"
      },
      "source": [
        "**Import python packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mysg2UEz0cu5"
      },
      "source": [
        "import copy\n",
        "import os\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert GTFS to GMNS Files"
      ],
      "metadata": {
        "id": "zbceNHC5gHXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_data(gtfs_path):\n",
        "    print('start reading input GTFS files...')\n",
        "    agency_df = _reading_text(gtfs_path + os.sep + 'agency')\n",
        "    agency_name = agency_df['agency_name'][0]\n",
        "    if '\"' in agency_name:\n",
        "        agency_name = eval(agency_name)\n",
        "        #  Remove quotes from string, for example '\"Arlington Transit\"' --> 'Arlington Transit'\n",
        "    print(\"agent_name:\", agency_name)\n",
        "\n",
        "    stop_df = _reading_text(gtfs_path + os.sep + 'stops')\n",
        "    stop_df = stop_df[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
        "    print(\"number of stops =\", len(stop_df))\n",
        "    #  select only latitude and longitude of the stops/stations\n",
        "\n",
        "    route_df = _reading_text(gtfs_path + os.sep + 'routes')\n",
        "    route_df = route_df[['route_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
        "    print(\"number of routes =\", len(route_df))\n",
        "\n",
        "    trip_df = _reading_text(gtfs_path + os.sep + 'trips')\n",
        "    if 'direction_id' not in trip_df.columns.tolist():  # direction_id is mandatory field name here\n",
        "        trip_df['direction_id'] = str(0)\n",
        "    trip_df['direction_id'] = trip_df.apply(lambda x: str(2 - int(x['direction_id'])), axis=1)\n",
        "    directed_route_id = trip_df['route_id'].astype(str).str.cat(\n",
        "        trip_df['direction_id'].astype(str), sep='.')\n",
        "    trip_df['directed_route_id'] = directed_route_id  # add a new column \"directed_route_id\"\n",
        "    #  If trips on a route service opposite directions,distinguish directions using values 0 and 1.\n",
        "    # revise the direction_id from 0,1 to 2,1\n",
        "    # add a new field directed_route_id\n",
        "    # deal with special issues of Agency 12 Fairfax CUE # Alicia, Nov 10:\n",
        "    # route file has route id with quotes, e.g., '\"green2\"' while trip file does not have it, e.g.,'green2'\n",
        "    if (route_df['route_id'][0][0] == '\"') != (trip_df['route_id'][0][0] == '\"'):\n",
        "        if route_df['route_id'][0][0] == '\"':\n",
        "            route_df['route_id'] = route_df.apply(lambda x: x['route_id'].strip('\"'), axis=1)\n",
        "        else:\n",
        "            trip_df['route_id'] = trip_df.apply(lambda x: x['route_id'].strip('\"'), axis=1)\n",
        "    trip_route_df = pd.merge(trip_df, route_df, on='route_id')  # might go wrong in Agency 12\n",
        "    print(\"number of trips =\", len(trip_route_df), \"...\", len(trip_df))\n",
        "    #  as route is higher level planning than trips, len(trip_route_df)=len(trip_df)\n",
        "\n",
        "    stop_time_df = _reading_text(gtfs_path + os.sep + 'stop_times')\n",
        "    print(\"number of stop_time records =\", len(stop_time_df))\n",
        "    # drop the stations without accurate arrival and departure time.\n",
        "\n",
        "    # drop nan\n",
        "    stop_time_df = stop_time_df.dropna(subset=['arrival_time'], how='any')\n",
        "    # drop ''\n",
        "    stop_time_df = stop_time_df[stop_time_df.arrival_time != '']\n",
        "    stop_time_df = stop_time_df[stop_time_df.departure_time != '']\n",
        "\n",
        "    # drop ' '\n",
        "    stop_time_df = stop_time_df[stop_time_df.arrival_time != ' ']\n",
        "    stop_time_df = stop_time_df[stop_time_df.departure_time != ' ']\n",
        "    print(\"number of stop_time records after dropping empty arrival and departure time =\", len(stop_time_df))\n",
        "\n",
        "    # convert timestamp to minute\n",
        "    # as some agencies might have trips overlapping two days, should use _to_timedelta to convert the data\n",
        "    print(\"start converting the time stamps...\")\n",
        "    tt = datetime.datetime(2021, 1, 1, 0, 0, 0, 0)\n",
        "    stop_time_df['arrival_time'] = pd.to_timedelta(stop_time_df['arrival_time']) + tt\n",
        "    stop_time_df['departure_time'] = pd.to_timedelta(stop_time_df['departure_time']) + tt\n",
        "    stop_time_df['arrival_time'] = \\\n",
        "        stop_time_df['arrival_time'].apply(lambda x: x.hour * 60 + x.minute + 1440 * (x.day - 1))\n",
        "    stop_time_df['departure_time'] = \\\n",
        "        stop_time_df['departure_time'].apply(lambda x: x.hour * 60 + x.minute + 1440 * (x.day - 1))\n",
        "\n",
        "    print(\"start marking terminal flags for stops...\")\n",
        "    iteration_group = stop_time_df.groupby(['trip_id'])\n",
        "    # mark terminal flag for each stop. The terminals can only be determined at the level of trips\n",
        "    input_list = []\n",
        "    time_start = time.time()\n",
        "    for trip_id, trip_stop_time_df in iteration_group:\n",
        "        trip_stop_time_df = trip_stop_time_df.sort_values(by=['stop_sequence'])\n",
        "        trip_stop_time_df = trip_stop_time_df.reset_index()\n",
        "        # select only the trips within the provided time window\n",
        "        if (trip_stop_time_df.arrival_time.min() <= period_end_time) & (\n",
        "                trip_stop_time_df.arrival_time.min() >= period_start_time):\n",
        "            input_list.append(trip_stop_time_df)\n",
        "    intermediate_output_list = list(map(_determine_terminal_flag, input_list))\n",
        "    output_list = list(map(_stop_sequence_label, intermediate_output_list))\n",
        "\n",
        "    # use map function to speed up marking process\n",
        "    time_end = time.time()\n",
        "    print('add terminal_flag for trips using CPU time:', time_end - time_start, 's')\n",
        "\n",
        "    time_start = time.time()\n",
        "    stop_time_df_with_terminal = pd.concat(output_list, axis=0)\n",
        "    # concatenating a list is much faster than concatenating separate dataframes\n",
        "    time_end = time.time()\n",
        "    print('concatenate different trips using CPU time:', time_end - time_start, 's')\n",
        "    print(\"have updated\", len(stop_time_df_with_terminal), \"stop_time records\")\n",
        "    print(\"merge the route information with trip information...\")\n",
        "    directed_trip_route_stop_time_df = pd.merge(trip_route_df, stop_time_df_with_terminal, on='trip_id')\n",
        "    print(\"number of final merged records =\", len(directed_trip_route_stop_time_df))\n",
        "    print(\"Data reading done..\")\n",
        "\n",
        "    #  as trip is higher level planning than stop time scheduling, len(stop_time_df)>=len(trip_df)\n",
        "    #  Each record of directed_trip_route_stop_time_df represents a space-time state of a vehicle\n",
        "    # trip_id (different vehicles, e.g., train lines)\n",
        "    # stop_id (spatial location of the vehicle)\n",
        "    # arrival_time,departure_time (time index of the vehicle)\n",
        "\n",
        "    directed_route_stop_id = directed_trip_route_stop_time_df['directed_route_id'].astype(\n",
        "        str).str.cat(directed_trip_route_stop_time_df['stop_id'].astype(str), sep='.')\n",
        "    directed_trip_route_stop_time_df['directed_route_stop_id'] = directed_route_stop_id\n",
        "    #  directed_route_stop_id is a unique id to identify the route, direction, and stop of a vehicle at a time point\n",
        "    directed_trip_route_stop_time_df['stop_sequence'] \\\n",
        "        = directed_trip_route_stop_time_df['stop_sequence'].astype('int32')\n",
        "    # two important concepts : 1 directed_service_stop_id (directed_route_stop_id + stop sequence)\n",
        "    directed_trip_route_stop_time_df['directed_service_stop_id'] = \\\n",
        "        directed_trip_route_stop_time_df.directed_route_stop_id.astype(str) + ':' + \\\n",
        "        directed_trip_route_stop_time_df.stop_sequence_label\n",
        "    # 2. directed service id (directed_route_id + stop sequence) same directed route id might have different sequences\n",
        "    directed_trip_route_stop_time_df['directed_service_id'] = \\\n",
        "        directed_trip_route_stop_time_df.directed_route_id.astype(str) + ':' + \\\n",
        "        directed_trip_route_stop_time_df.stop_sequence_label\n",
        "    #  attach stop name and geometry for stops\n",
        "    directed_trip_route_stop_time_df = pd.merge(directed_trip_route_stop_time_df, stop_df, on='stop_id')\n",
        "    directed_trip_route_stop_time_df['agency_name'] = agency_name\n",
        "\n",
        "    return stop_df, route_df, trip_df, trip_route_df, stop_time_df, directed_trip_route_stop_time_df\n",
        "\n",
        "\n",
        "def create_nodes(directed_trip_route_stop_time_df, agency_num):\n",
        "    \"\"\"create physical (station) node...\"\"\"\n",
        "    physical_node_df = pd.DataFrame()\n",
        "    temp_df = directed_trip_route_stop_time_df.drop_duplicates(subset=['stop_id'])\n",
        "    physical_node_df['name'] = temp_df['stop_id']\n",
        "    physical_node_df = physical_node_df.sort_values(by=['name'])\n",
        "    physical_node_df['node_id'] = \\\n",
        "        np.linspace(start=1, stop=len(physical_node_df), num=len(physical_node_df)).astype('int32')\n",
        "    physical_node_df['node_id'] += int('{}000000'.format(agency_num))\n",
        "    physical_node_df['physical_node_id'] = physical_node_df['node_id']\n",
        "    physical_node_df['x_coord'] = temp_df['stop_lon'].astype(float)\n",
        "    physical_node_df['y_coord'] = temp_df['stop_lat'].astype(float)\n",
        "    physical_node_df['route_type'] = temp_df['route_type']\n",
        "    physical_node_df['route_id'] = temp_df['route_id']\n",
        "    physical_node_df['node_type'] = \\\n",
        "        physical_node_df.apply(lambda x: _convert_route_type_to_node_type_p(x.route_type), axis=1)\n",
        "    physical_node_df['directed_route_id'] = \"\"\n",
        "    physical_node_df['directed_service_id'] = \"\"\n",
        "    physical_node_df['zone_id'] = \"\"\n",
        "    physical_node_df['agency_name'] = temp_df['agency_name']\n",
        "    physical_node_df['geometry'] = 'POINT (' + physical_node_df['x_coord'].astype(str) + \\\n",
        "                                   ' ' + physical_node_df['y_coord'].astype(str) + ')'\n",
        "    stop_name_id_dict = dict(zip(physical_node_df['name'], physical_node_df['node_id']))\n",
        "    physical_node_df['terminal_flag'] = temp_df['terminal_flag']\n",
        "    physical_node_df['ctrl_type'] = \"\"\n",
        "    physical_node_df['agent_type'] = \"\"\n",
        "\n",
        "    \"\"\" create service node...\"\"\"\n",
        "    service_node_df = pd.DataFrame()\n",
        "    temp_df = directed_trip_route_stop_time_df.drop_duplicates(subset=['directed_service_stop_id'])\n",
        "    # 2.2.2 route stop node\n",
        "    service_node_df['name'] = temp_df['directed_service_stop_id']\n",
        "    service_node_df = service_node_df.sort_values(by=['name'])\n",
        "    service_node_df['node_id'] = \\\n",
        "        np.linspace(start=1, stop=len(service_node_df), num=len(service_node_df)).astype('int32')\n",
        "    service_node_df['physical_node_id'] = temp_df.apply(lambda x: stop_name_id_dict[x.stop_id], axis=1)\n",
        "    service_node_df['node_id'] += int('{}500000'.format(agency_num))\n",
        "\n",
        "    service_node_df['x_coord'] = temp_df['stop_lon'].astype(float) - 0.000100\n",
        "    service_node_df['y_coord'] = temp_df['stop_lat'].astype(float) - 0.000100\n",
        "    service_node_df['route_type'] = temp_df['route_type']\n",
        "    service_node_df['route_id'] = temp_df['route_id']\n",
        "    service_node_df['node_type'] = \\\n",
        "        service_node_df.apply(lambda x: _convert_route_type_to_node_type_s(x.route_type), axis=1)\n",
        "    # node_csv['terminal_flag'] = ' '\n",
        "    service_node_df['directed_route_id'] = temp_df['directed_route_id'].astype(str)\n",
        "    service_node_df['directed_service_id'] = temp_df['directed_service_id'].astype(str)\n",
        "    service_node_df['zone_id'] = \"\"\n",
        "    service_node_df['agency_name'] = temp_df['agency_name']\n",
        "    service_node_df['geometry'] = \\\n",
        "        'POINT (' + service_node_df['x_coord'].astype(str) + ' ' + service_node_df['y_coord'].astype(str) + ')'\n",
        "\n",
        "    service_node_df['terminal_flag'] = temp_df['terminal_flag']\n",
        "    service_node_df['ctrl_type'] = \"\"\n",
        "    service_node_df['agent_type'] = \"\"\n",
        "    # concatenate service and physical node\n",
        "    node_df = pd.concat([physical_node_df, service_node_df])\n",
        "    return node_df\n",
        "\n",
        "\n",
        "def create_service_boarding_links(directed_trip_route_stop_time_df, node_df, agency_num, one_agency_link_list):\n",
        "    \"\"\"dictionaries\"\"\"\n",
        "    node_id_dict = dict(zip(node_df['name'], node_df['node_id']))\n",
        "    directed_service_dict = dict(zip(node_df['node_id'], node_df['name']))\n",
        "    node_lon_dict = dict(zip(node_df['node_id'], node_df['x_coord']))\n",
        "    node_lat_dict = dict(zip(node_df['node_id'], node_df['y_coord']))\n",
        "    frequency_dict = {}\n",
        "\n",
        "    print(\"1. start creating route links...\")\n",
        "    \"\"\"service links\"\"\"\n",
        "    number_of_route_links = 0\n",
        "    iteration_group = directed_trip_route_stop_time_df.groupby('directed_service_id')\n",
        "    labeled_directed_service_list = []\n",
        "\n",
        "    time_start = time.time()\n",
        "    for directed_service_id, route_df in iteration_group:\n",
        "        if directed_service_id in labeled_directed_service_list:\n",
        "            continue\n",
        "        else:\n",
        "            labeled_directed_service_list.append(directed_service_id)\n",
        "            number_of_trips = len(route_df.trip_id.unique())\n",
        "            frequency_dict[directed_service_id] = number_of_trips  # note the frequency of routes\n",
        "            one_line_df = route_df[route_df.trip_id == route_df.trip_id.unique()[0]]\n",
        "            one_line_df = one_line_df.sort_values(by=['stop_sequence'])\n",
        "            number_of_records = len(one_line_df)\n",
        "            one_line_df = one_line_df.reset_index()\n",
        "            for k in range(number_of_records - 1):\n",
        "                link_id = 1000000 * agency_num + number_of_route_links + 1\n",
        "                from_node_id = node_id_dict[one_line_df.iloc[k].directed_service_stop_id]\n",
        "                to_node_id = node_id_dict[one_line_df.iloc[k + 1].directed_service_stop_id]\n",
        "                facility_type = _convert_route_type_to_link_type(one_line_df.iloc[k].route_type)\n",
        "                dir_flag = 1\n",
        "                directed_route_id = one_line_df.iloc[k].directed_route_id\n",
        "                link_type = 1\n",
        "                link_type_name = 'service_links'\n",
        "                from_node_lon = float(one_line_df.iloc[k].stop_lon)\n",
        "                from_node_lat = float(one_line_df.iloc[k].stop_lat)\n",
        "                to_node_lon = float(one_line_df.iloc[k + 1].stop_lon)\n",
        "                to_node_lat = float(one_line_df.iloc[k + 1].stop_lat)\n",
        "                length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "                lanes = number_of_trips\n",
        "                capacity = 999999\n",
        "                VDF_fftt1 = one_line_df.iloc[k + 1].arrival_time - one_line_df.iloc[k].arrival_time\n",
        "                # minutes\n",
        "                VDF_cap1 = lanes * capacity\n",
        "                free_speed = ((length / 1000) / (VDF_fftt1 + 0.001)) * 60\n",
        "                # (kilometers/minutes)*60 = kilometer/hour\n",
        "                VDF_alpha1 = 0.15\n",
        "                VDF_beta1 = 4\n",
        "                VDF_penalty1 = 0\n",
        "                cost = 0\n",
        "                geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                           str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "                agency_name = one_line_df.agency_name[0]\n",
        "                allowed_use = _allowed_use_function(one_line_df.iloc[k].route_type)\n",
        "                stop_sequence = one_line_df.iloc[k].stop_sequence\n",
        "                directed_service_id = one_line_df.iloc[k].directed_service_id\n",
        "                link_list = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                             link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                             VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                             agency_name,\n",
        "                             stop_sequence, directed_service_id]\n",
        "                one_agency_link_list.append(link_list)\n",
        "                number_of_route_links += 1\n",
        "                if number_of_route_links % 50 == 0:\n",
        "                    time_end = time.time()\n",
        "                    print('convert ', number_of_route_links,\n",
        "                          'service links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    print(\"2. start creating boarding links from stations to their passing routes...\")\n",
        "    \"\"\"boarding_links\"\"\"\n",
        "    service_node_df = node_df[node_df.node_id != node_df.physical_node_id]\n",
        "    #  select service node from node_df\n",
        "    service_node_df = service_node_df.reset_index()\n",
        "    number_of_sta2route_links = 0\n",
        "    for iter, row in service_node_df.iterrows():\n",
        "        link_id = agency_num * 1000000 + number_of_route_links + number_of_sta2route_links\n",
        "        from_node_id = row.physical_node_id\n",
        "        to_node_id = row.node_id\n",
        "        facility_type = _convert_route_type_to_link_type(row.route_type)\n",
        "        dir_flag = 1\n",
        "        directed_route_id = row.directed_route_id\n",
        "        link_type = 2\n",
        "        link_type_name = 'boarding_links'\n",
        "        to_node_lon = row.x_coord\n",
        "        to_node_lat = row.y_coord\n",
        "        from_node_lon = node_lon_dict[row.physical_node_id]\n",
        "        from_node_lat = node_lat_dict[row.physical_node_id]\n",
        "        length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "        free_speed = 2\n",
        "        lanes = 1\n",
        "        capacity = 999999\n",
        "        VDF_cap1 = lanes * capacity\n",
        "        VDF_alpha1 = 0.15\n",
        "        VDF_beta1 = 4\n",
        "        VDF_penalty1 = 0\n",
        "        cost = 0\n",
        "        stop_sequence = -1\n",
        "        directed_service_id = directed_service_dict[to_node_id]\n",
        "        geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                   str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "        agency_name = row.agency_name\n",
        "        allowed_use = _allowed_use_function(row.route_type)\n",
        "\n",
        "        # inbound links (boarding)\n",
        "\n",
        "        VDF_fftt1 = \\\n",
        "            0.5 * ((period_end_time - period_start_time) / frequency_dict[row.directed_service_id])\n",
        "        VDF_fftt1 = min(VDF_fftt1, 10)\n",
        "        # waiting time at a station is 10 minutes at most\n",
        "        geometry = 'LINESTRING (' + str(to_node_lon) + ' ' + str(to_node_lat) + ', ' + \\\n",
        "                   str(from_node_lon) + ' ' + str(from_node_lat) + ')'\n",
        "        # inbound link is average waiting time derived from frequency\n",
        "        link_list_inbound = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                             link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                             VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                             agency_name,\n",
        "                             stop_sequence, directed_service_id]\n",
        "        number_of_sta2route_links += 1\n",
        "\n",
        "        # outbound links (boarding)\n",
        "        link_id = agency_num * 1000000 + number_of_route_links + number_of_sta2route_links\n",
        "        VDF_fftt1 = 1  # (length / free_speed) * 60\n",
        "        #  the time of outbound time\n",
        "        link_list_outbound = [link_id, to_node_id, from_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                              link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                              VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                              agency_name,\n",
        "                              stop_sequence, directed_service_id]\n",
        "        one_agency_link_list.append(link_list_inbound)\n",
        "        one_agency_link_list.append(link_list_outbound)\n",
        "        number_of_sta2route_links += 1\n",
        "        #  one inbound link and one outbound link\n",
        "        if number_of_sta2route_links % 50 == 0:\n",
        "            time_end = time.time()\n",
        "            print('convert ', number_of_sta2route_links,\n",
        "                  'boarding links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    return one_agency_link_list\n",
        "\n",
        "\n",
        "def create_transferring_links(all_node_df, all_link_list):\n",
        "    physical_node_df = all_node_df[all_node_df.node_id == all_node_df.physical_node_id]\n",
        "    physical_node_df = physical_node_df.reset_index()\n",
        "    number_of_transferring_links = 0\n",
        "    time_start = time.time()\n",
        "    for i in range(len(physical_node_df)):\n",
        "        ref_x = physical_node_df.iloc[i].x_coord\n",
        "        ref_y = physical_node_df.iloc[i].y_coord\n",
        "        neighboring_node_df = physical_node_df[(physical_node_df.x_coord >= (ref_x - 0.003)) &\n",
        "                                               (physical_node_df.x_coord <= (ref_x + 0.003))]\n",
        "        neighboring_node_df = neighboring_node_df[(neighboring_node_df.y_coord >= (ref_y - 0.003)) &\n",
        "                                                  (neighboring_node_df.y_coord <= (ref_y + 0.003))]\n",
        "        labeled_list = []\n",
        "        count = 0\n",
        "        for j in range(len(neighboring_node_df)):\n",
        "            if count >= 10:\n",
        "                break\n",
        "            if (physical_node_df.iloc[i].route_id, physical_node_df.iloc[i].agency_name) == \\\n",
        "                    (neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name):\n",
        "                continue\n",
        "            from_node_lon = float(physical_node_df.iloc[i].x_coord)\n",
        "            from_node_lat = float(physical_node_df.iloc[i].y_coord)\n",
        "            to_node_lon = float(neighboring_node_df.iloc[j].x_coord)\n",
        "            to_node_lat = float(neighboring_node_df.iloc[j].y_coord)\n",
        "            length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "            if (length > 321.869) | (length < 1):\n",
        "                continue\n",
        "            if (neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name) in labeled_list:\n",
        "                continue\n",
        "            count += 1\n",
        "            labeled_list.append((neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name))\n",
        "            # consider only one stops of another route\n",
        "            # transferring 1\n",
        "            #  print('transferring link length =', length)\n",
        "            link_id = number_of_transferring_links + 1\n",
        "            from_node_id = physical_node_df.iloc[i].node_id\n",
        "            to_node_id = neighboring_node_df.iloc[j].node_id\n",
        "            facility_type = 'sta2sta'\n",
        "            dir_flag = 1\n",
        "            directed_route_id = -1\n",
        "            link_type = 3\n",
        "            link_type_name = 'transferring_links'\n",
        "            lanes = 1\n",
        "            capacity = 999999\n",
        "            VDF_fftt1 = (length / 1000) / 1\n",
        "            VDF_cap1 = lanes * capacity\n",
        "            free_speed = 1\n",
        "            # 1 kilo/hour\n",
        "            VDF_alpha1 = 0.15\n",
        "            VDF_beta1 = 4\n",
        "            VDF_penalty1 = _transferring_penalty(physical_node_df.iloc[i].node_type, neighboring_node_df.iloc[j].node_type)\n",
        "            # penalty of transferring\n",
        "            cost = 60\n",
        "            geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                       str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "            agency_name = \"\"\n",
        "            allowed_use = \\\n",
        "                _allowed_use_transferring(physical_node_df.iloc[i].node_type, neighboring_node_df.iloc[j].node_type)\n",
        "            stop_sequence = \"\"\n",
        "            directed_service_id = \"\"\n",
        "            link_list = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                         link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                         VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use, agency_name,\n",
        "                         stop_sequence, directed_service_id]\n",
        "            all_link_list.append(link_list)\n",
        "            # transferring 2\n",
        "            number_of_transferring_links += 1\n",
        "            geometry = 'LINESTRING (' + str(to_node_lon) + ' ' + str(to_node_lat) + ', ' + \\\n",
        "                       str(from_node_lon) + ' ' + str(from_node_lat) + ')'\n",
        "            link_id = number_of_transferring_links + 1\n",
        "            link_list = [link_id, to_node_id, from_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                         link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                         VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use, agency_name,\n",
        "                         stop_sequence, directed_service_id]\n",
        "            all_link_list.append(link_list)\n",
        "            number_of_transferring_links += 1\n",
        "            if number_of_transferring_links % 50 == 0:\n",
        "                time_end = time.time()\n",
        "                print('convert ', number_of_transferring_links,\n",
        "                      'transferring links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    return all_link_list\n",
        "\n",
        "\n",
        "\"\"\" ------------------functions------------------ \"\"\"\n",
        "\n",
        "\n",
        "def _stop_sequence_label(trip_stop_time_df):\n",
        "    trip_stop_time_df = trip_stop_time_df.sort_values(by=['stop_sequence'])\n",
        "    trip_stop_time_df['stop_sequence_label'] = ';'.join(np.array(trip_stop_time_df.stop_sequence).astype(str))\n",
        "    return trip_stop_time_df\n",
        "\n",
        "\n",
        "def _reading_text(filename):\n",
        "    file_path = filename + '.txt'\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
        "        lines = f.readlines()\n",
        "        first_line = lines[0].split('\\n')[0].split(',')\n",
        "        for line in lines:\n",
        "            if len(line.split('\\n')[0].split(',')) == len(first_line):\n",
        "                data.append(line.split('\\n')[0].split(','))\n",
        "            else:\n",
        "                data.append(_split_ignore_separators_in_quoted(line))\n",
        "    data_frame = pd.DataFrame(data[1:], columns=data[0])\n",
        "    return data_frame\n",
        "\n",
        "\n",
        "def _determine_terminal_flag(trip_stop_time_df):\n",
        "    trip_stop_time_df.stop_sequence = trip_stop_time_df.stop_sequence.astype('int32')\n",
        "    start_stop_seq = int(trip_stop_time_df.stop_sequence.min())\n",
        "    end_stop_seq = int(trip_stop_time_df.stop_sequence.max())\n",
        "    #  convert string to integer\n",
        "    trip_stop_time_df['terminal_flag'] = \\\n",
        "        ((trip_stop_time_df.stop_sequence == start_stop_seq) |\n",
        "         (trip_stop_time_df.stop_sequence == end_stop_seq)).astype('int32')\n",
        "    return trip_stop_time_df\n",
        "\n",
        "\n",
        "def _allowed_use_function(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    allowed_use = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        allowed_use = \"w_bus_only;w_bus_metro;d_bus_only;d_bus_metro\"\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        allowed_use = \"w_metro_only;w_bus_metro;d_metro_only;d_bus_metro\"\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        allowed_use = \"w_rail_only;d_rail_only\"\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        allowed_use = \"w_bus_only;w_bus_metro;d_bus_only;d_bus_metro\"\n",
        "    return allowed_use\n",
        "\n",
        "\n",
        "def _allowed_use_transferring(node_type_1, node_type_2):\n",
        "    if (node_type_1 == 'stop') & (node_type_2 == 'stop'):\n",
        "        allowed_use = \"w_bus_only;d_bus_only\"\n",
        "    elif (node_type_1 == 'stop') & (node_type_2 == 'metro_station'):\n",
        "        allowed_use = \"w_bus_metro;d_bus_metro\"\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'stop'):\n",
        "        allowed_use = \"w_bus_metro;d_bus_metro\"\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'metro_station'):\n",
        "        allowed_use = \"w_metro_only;d_metro_only\"\n",
        "    elif (node_type_1 == 'rail_station') & (node_type_2 == 'rail_station'):\n",
        "        allowed_use = \"w_rail_only;d_rail_only\"\n",
        "    else:\n",
        "        allowed_use = \"closed\"\n",
        "\n",
        "    return allowed_use\n",
        "\n",
        "\n",
        "def _transferring_penalty(node_type_1, node_type_2):\n",
        "    if (node_type_1 == 'stop') & (node_type_2 == 'stop'):\n",
        "        VDF_penalty1 = 99\n",
        "    elif (node_type_1 == 'stop') & (node_type_2 == 'metro_station'):\n",
        "        VDF_penalty1 = 0\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'stop'):\n",
        "        VDF_penalty1 = 0\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'metro_station'):\n",
        "        VDF_penalty1 = 99\n",
        "    elif (node_type_1 == 'rail_station') & (node_type_2 == 'rail_station'):\n",
        "        VDF_penalty1 = 99\n",
        "    else:\n",
        "        VDF_penalty1 = 1000\n",
        "\n",
        "    return VDF_penalty1\n",
        "\n",
        "\n",
        "def _convert_route_type_to_node_type_p(route_type):\n",
        "    #  convert route type to node type on physical network\n",
        "    node_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        node_type = 'stop'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        node_type = 'metro_station'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        node_type = 'rail_station'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        node_type = 'stop'\n",
        "    return node_type\n",
        "\n",
        "\n",
        "def _convert_route_type_to_node_type_s(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    node_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        node_type = 'tram_service_node'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        node_type = 'metro_service_node'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        node_type = 'rail_service_node'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        node_type = 'bus_service_node'\n",
        "    return node_type\n",
        "\n",
        "\n",
        "def _convert_route_type_to_link_type(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    link_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        link_type = 'tram'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        link_type = 'metro'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        link_type = 'rail'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        link_type = 'bus'\n",
        "    return link_type\n",
        "\n",
        "\n",
        "def _split_ignore_separators_in_quoted(s, separator=',', quote_mark='\"'):\n",
        "    result = []\n",
        "    quoted = False\n",
        "    current = ''\n",
        "    for i in range(len(s)):\n",
        "        if quoted:\n",
        "            current += s[i]\n",
        "            if s[i] == quote_mark:\n",
        "                quoted = False\n",
        "            continue\n",
        "        if s[i] == separator:\n",
        "            result.append(current.strip())\n",
        "            current = ''\n",
        "        else:\n",
        "            current += s[i]\n",
        "            if s[i] == quote_mark:\n",
        "                quoted = True\n",
        "    result.append(current)\n",
        "    return result\n",
        "\n",
        "\n",
        "def _calculate_distance_from_geometry(lon1, lat1, lon2, lat2):  # WGS84 transfer coordinate system to distance(mile) #xy\n",
        "    radius = 6371\n",
        "    d_latitude = (lat2 - lat1) * math.pi / 180.0\n",
        "    d_longitude = (lon2 - lon1) * math.pi / 180.0\n",
        "\n",
        "    a = math.sin(d_latitude / 2) * math.sin(d_latitude / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(\n",
        "        lat2 * math.pi / 180.0) * math.sin(d_longitude / 2) * math.sin(d_longitude / 2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "    # distance = radius * c * 1000 / 1609.34  # mile\n",
        "    distance = radius * c * 1000  # meter\n",
        "    return distance\n",
        "\n",
        "\n",
        "def _hhmm_to_minutes(time_period_1):\n",
        "    from_time_1 = datetime.time(int(time_period_1[0:2]), int(time_period_1[2:4]))\n",
        "    to_time_1 = datetime.time(int(time_period_1[-4:-2]), int(time_period_1[-2:]))\n",
        "    from_time_min_1 = from_time_1.hour * 60 + from_time_1.minute\n",
        "    to_time_min_1 = to_time_1.hour * 60 + to_time_1.minute\n",
        "    return from_time_min_1, to_time_min_1\n",
        "\n",
        "\n",
        "\"\"\" ------------------main functions------------------ \"\"\"\n",
        "\n",
        "\n",
        "def gtfs2gmns(input_path, output_path):\n",
        "    start_time = time.time()\n",
        "    folders = os.listdir(input_path)\n",
        "    gtfs_folder_list = []\n",
        "    for sub_folder in folders:\n",
        "        sub_folder_path = input_path + '/' + sub_folder\n",
        "        if os.path.isdir(sub_folder_path):  # check whether the specified path is an existing directory or not.\n",
        "            gtfs_folder_list.append(sub_folder_path)\n",
        "    if len(gtfs_folder_list) == 0:\n",
        "        gtfs_folder_list.append(input_path)\n",
        "\n",
        "    all_node_list = []\n",
        "    all_link_list = []\n",
        "    for i in range(len(gtfs_folder_list)):\n",
        "        print('Start converting Agency_{}...'.format(i + 1))\n",
        "        print('Directory : ' + str(gtfs_folder_list[i]))\n",
        "        agency_gtfs_path = gtfs_folder_list[i]\n",
        "        \"\"\" step 1. reading data \"\"\"\n",
        "        stop_df, route_df, trip_df, trip_route_df, stop_time_df, directed_trip_route_stop_time_df = \\\n",
        "            reading_data(agency_gtfs_path)\n",
        "        #  directed_trip_route_stop_time_df.to_csv(gtfs_folder_list[i] + '/timetable.csv', index=False)\n",
        "        #  directed_trip_route_stop_time_df = pd.read_csv(gtfs_folder_list[i] + '/timetable.csv')\n",
        "\n",
        "        \"\"\"step 2. create nodes\"\"\"\n",
        "        agency_num = i + 1\n",
        "        # number of agency equals to i+1\n",
        "        node_df = create_nodes(directed_trip_route_stop_time_df, agency_num)\n",
        "        all_node_list.append(node_df)\n",
        "        print(\"node.csv of\", str(gtfs_folder_list[i]), \"has been generated...\")\n",
        "        \"\"\"step 3. create links\"\"\"\n",
        "        all_link_list \\\n",
        "            = create_service_boarding_links(directed_trip_route_stop_time_df, node_df, agency_num, all_link_list)\n",
        "\n",
        "        if i == len(gtfs_folder_list):\n",
        "            print('output')\n",
        "        print('Conversion of  Agency{}...'.format(agency_num + 1), ' have done..')\n",
        "\n",
        "    all_node_df = pd.concat(all_node_list)\n",
        "    all_node_df.reset_index(inplace=True)\n",
        "    all_node_df = all_node_df.drop(['index'], axis=1)\n",
        "    # transferring links\n",
        "    all_link_list = create_transferring_links(all_node_df, all_link_list)\n",
        "\n",
        "    all_link_df = pd.DataFrame(all_link_list)\n",
        "    all_link_df.rename(columns={0: 'link_id',\n",
        "                                1: 'from_node_id',\n",
        "                                2: 'to_node_id',\n",
        "                                3: 'facility_type',\n",
        "                                4: 'dir_flag',\n",
        "                                5: 'directed_route_id',\n",
        "                                6: 'link_type',\n",
        "                                7: 'link_type_name',\n",
        "                                8: 'length',\n",
        "                                9: 'lanes',\n",
        "                                10: 'capacity',\n",
        "                                11: 'free_speed',\n",
        "                                12: 'cost',\n",
        "                                13: 'VDF_fftt1',\n",
        "                                14: 'VDF_cap1',\n",
        "                                15: 'VDF_alpha1',\n",
        "                                16: 'VDF_beta1',\n",
        "                                17: 'VDF_penalty1',\n",
        "                                18: 'geometry',\n",
        "                                19: 'VDF_allowed_uses1',\n",
        "                                20: 'agency_name',\n",
        "                                21: 'stop_sequence',\n",
        "                                22: 'directed_service_id'}, inplace=True)\n",
        "    all_node_df.to_csv('node.csv', index=False)\n",
        "    #  zone_df = pd.read_csv('zone.csv')\n",
        "    #  source_node_df = pd.read_csv('source_node.csv')\n",
        "    #  node_df = pd.concat([zone_df, all_node_df])\n",
        "    #  node_df.to_csv(\"node.csv\", index=False)\n",
        "    all_link_df = all_link_df.drop_duplicates(\n",
        "        subset=['from_node_id', 'to_node_id'],\n",
        "        keep='last').reset_index(drop=True)\n",
        "    all_link_df.to_csv('link.csv', index=False)\n",
        "    print('run time -->', time.time() - start_time)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    global period_start_time\n",
        "    global period_end_time\n",
        "    input_path = '/content/GTFS2GMNS/test'\n",
        "    output_path = '/content/GTFS2GMNS/test'\n",
        "    time_period_id = 1\n",
        "    time_period = '0700_0800'\n",
        "    period_start_time, period_end_time = _hhmm_to_minutes(time_period)\n",
        "\n",
        "    gtfs2gmns(input_path, output_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "TfHTwVuXhVHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2baaff5-7665-4f2f-aec1-77026ede245b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start converting Agency_1...\n",
            "Directory : /content/GTFS2GMNS/test/Flagstaff_MountainLine\n",
            "start reading input GTFS files...\n",
            "agent_name: Mountain Line\n",
            "number of stops = 180\n",
            "number of routes = 9\n",
            "number of trips = 488 ... 488\n",
            "number of stop_time records = 10239\n",
            "number of stop_time records after dropping empty arrival and departure time = 3132\n",
            "start converting the time stamps...\n",
            "start marking terminal flags for stops...\n",
            "add terminal_flag for trips using CPU time: 0.7508823871612549 s\n",
            "concatenate different trips using CPU time: 0.016277074813842773 s\n",
            "have updated 230 stop_time records\n",
            "merge the route information with trip information...\n",
            "number of final merged records = 230\n",
            "Data reading done..\n",
            "node.csv of /content/GTFS2GMNS/test/Flagstaff_MountainLine has been generated...\n",
            "1. start creating route links...\n",
            "convert  50 service links successfully... using time 0.10982251167297363 s\n",
            "convert  100 service links successfully... using time 0.2437572479248047 s\n",
            "2. start creating boarding links from stations to their passing routes...\n",
            "convert  50 boarding links successfully... using time 0.2690253257751465 s\n",
            "convert  100 boarding links successfully... using time 0.2767314910888672 s\n",
            "convert  150 boarding links successfully... using time 0.2814595699310303 s\n",
            "convert  200 boarding links successfully... using time 0.2856769561767578 s\n",
            "Conversion of  Agency2...  have done..\n",
            "Start converting Agency_2...\n",
            "Directory : /content/GTFS2GMNS/test/Cottonwood_Area_Transit\n",
            "start reading input GTFS files...\n",
            "agent_name: Cottonwood Area Transit\n",
            "number of stops = 141\n",
            "number of routes = 6\n",
            "number of trips = 84 ... 84\n",
            "number of stop_time records = 2268\n",
            "number of stop_time records after dropping empty arrival and departure time = 2268\n",
            "start converting the time stamps...\n",
            "start marking terminal flags for stops...\n",
            "add terminal_flag for trips using CPU time: 0.11809849739074707 s\n",
            "concatenate different trips using CPU time: 0.007141828536987305 s\n",
            "have updated 183 stop_time records\n",
            "merge the route information with trip information...\n",
            "number of final merged records = 183\n",
            "Data reading done..\n",
            "node.csv of /content/GTFS2GMNS/test/Cottonwood_Area_Transit has been generated...\n",
            "1. start creating route links...\n",
            "convert  50 service links successfully... using time 0.1671452522277832 s\n",
            "convert  100 service links successfully... using time 0.3457984924316406 s\n",
            "2. start creating boarding links from stations to their passing routes...\n",
            "convert  50 boarding links successfully... using time 0.629474401473999 s\n",
            "convert  100 boarding links successfully... using time 0.6443948745727539 s\n",
            "convert  150 boarding links successfully... using time 0.6552915573120117 s\n",
            "convert  200 boarding links successfully... using time 0.6666393280029297 s\n",
            "convert  250 boarding links successfully... using time 0.678342342376709 s\n",
            "Conversion of  Agency3...  have done..\n",
            "Start converting Agency_3...\n",
            "Directory : /content/GTFS2GMNS/test/Sedona_RoadRunner\n",
            "start reading input GTFS files...\n",
            "agent_name: RoadRunner\n",
            "number of stops = 169\n",
            "number of routes = 10\n",
            "number of trips = 45 ... 45\n",
            "number of stop_time records = 698\n",
            "number of stop_time records after dropping empty arrival and departure time = 268\n",
            "start converting the time stamps...\n",
            "start marking terminal flags for stops...\n",
            "add terminal_flag for trips using CPU time: 0.23283767700195312 s\n",
            "concatenate different trips using CPU time: 0.022437572479248047 s\n",
            "have updated 92 stop_time records\n",
            "merge the route information with trip information...\n",
            "number of final merged records = 92\n",
            "Data reading done..\n",
            "node.csv of /content/GTFS2GMNS/test/Sedona_RoadRunner has been generated...\n",
            "1. start creating route links...\n",
            "convert  50 service links successfully... using time 0.2887272834777832 s\n",
            "2. start creating boarding links from stations to their passing routes...\n",
            "convert  50 boarding links successfully... using time 0.43877387046813965 s\n",
            "convert  100 boarding links successfully... using time 0.45326924324035645 s\n",
            "Conversion of  Agency4...  have done..\n",
            "convert  50 transferring links successfully... using time 0.349287748336792 s\n",
            "convert  100 transferring links successfully... using time 0.7119858264923096 s\n",
            "convert  150 transferring links successfully... using time 1.5925233364105225 s\n",
            "convert  200 transferring links successfully... using time 3.0098094940185547 s\n",
            "convert  250 transferring links successfully... using time 3.371530771255493 s\n",
            "convert  300 transferring links successfully... using time 3.7024459838867188 s\n",
            "run time --> 7.359015703201294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download GMNS data files"
      ],
      "metadata": {
        "id": "0mw8Jzv2ikXI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDf0i_KS7dIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ba39d811-f6dd-420a-e068-e8f1f9935a18"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/GTFS2GMNS/test/node.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_61daa02a-2f9e-41a1-9187-fe07e74ee5f0\", \"node.csv\", 104018)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/GTFS2GMNS/test/link.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OK7TwXb0pZNf",
        "outputId": "63faad12-1894-4cac-b5a4-7f385f2a9b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0a13c6b0-05b4-415f-9281-e9bce6754994\", \"link.csv\", 330849)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6pyZEum8Ood"
      },
      "source": [
        "**Visualization using GMNS tool:**\n",
        "By simply uploading node.csv and link.csv at https://asu-trans-ai-lab.github.io/index.html#,  \n",
        " you can easily create custom online maps for any GMNS network files. \n",
        "To view zone and demand information please visit this page to use QGIS/NeXTA tools. https://github.com/asu-trans-ai-lab/traffic-engineering-and-analysis/blob/master/undergraduate_student_project/QGIS%20For%20Gmns%20User%20Guide_v0.5.pdf "
      ]
    }
  ]
}