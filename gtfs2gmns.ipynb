{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gtfs2gmns.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asu-trans-ai-lab/GTFS2GMNS/blob/main/gtfs2gmns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparation#"
      ],
      "metadata": {
        "id": "7cCSGNLJf3z7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ghEaf-d2Fdm"
      },
      "source": [
        "**Step 0.1: load the GTFS file from the repository of GTFS testing datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3xZMKsJ2Ew2",
        "outputId": "6cec94b0-8de2-4b65-c61d-e4173ac28758"
      },
      "source": [
        "!rm -rf ./GTFS2GMNS/\n",
        "!git clone https://github.com/asu-trans-ai-lab/GTFS2GMNS\n",
        "\n",
        "%cd GTFS2GMNS/test/GTFS"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GTFS2GMNS'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 282 (delta 63), reused 18 (delta 1), pack-reused 92\u001b[K\n",
            "Receiving objects: 100% (282/282), 275.19 MiB | 22.82 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "Checking out files: 100% (26/26), done.\n",
            "/content/GTFS2GMNS/test/GTFS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CexwTGDB0D0A"
      },
      "source": [
        "Check the file icon on the left hand side, makesure files stop.txt, route.txt, trip.txt, stop_times.txt, agency.txt exist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6y7B8WX-d46"
      },
      "source": [
        "**Step 0.2: Import python packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mysg2UEz0cu5"
      },
      "source": [
        "import copy\n",
        "import os\n",
        "import math\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert GTFS to GMNS Files#"
      ],
      "metadata": {
        "id": "zbceNHC5gHXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Basic Functions**"
      ],
      "metadata": {
        "id": "tczRYvoslJVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _stop_sequence_label(trip_stop_time_df):\n",
        "    trip_stop_time_df = trip_stop_time_df.sort_values(by=['stop_sequence'])\n",
        "    trip_stop_time_df['stop_sequence_label'] = ';'.join(np.array(trip_stop_time_df.stop_sequence).astype(str))\n",
        "    return trip_stop_time_df\n",
        "\n",
        "\n",
        "def _reading_text(filename):\n",
        "    file_path = filename + '.txt'\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
        "        lines = f.readlines()\n",
        "        first_line = lines[0].split('\\n')[0].split(',')\n",
        "        for line in lines:\n",
        "            if len(line.split('\\n')[0].split(',')) == len(first_line):\n",
        "                data.append(line.split('\\n')[0].split(','))\n",
        "            else:\n",
        "                data.append(_split_ignore_separators_in_quoted(line))\n",
        "    data_frame = pd.DataFrame(data[1:], columns=data[0])\n",
        "    return data_frame\n",
        "\n",
        "\n",
        "def _determine_terminal_flag(trip_stop_time_df):\n",
        "    trip_stop_time_df.stop_sequence = trip_stop_time_df.stop_sequence.astype('int32')\n",
        "    start_stop_seq = int(trip_stop_time_df.stop_sequence.min())\n",
        "    end_stop_seq = int(trip_stop_time_df.stop_sequence.max())\n",
        "    #  convert string to integer\n",
        "    trip_stop_time_df['terminal_flag'] = \\\n",
        "        ((trip_stop_time_df.stop_sequence == start_stop_seq) |\n",
        "         (trip_stop_time_df.stop_sequence == end_stop_seq)).astype('int32')\n",
        "    return trip_stop_time_df\n",
        "\n",
        "\n",
        "def _allowed_use_function(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    allowed_use = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        allowed_use = \"w_bus_only;w_bus_metro;d_bus_only;d_bus_metro\"\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        allowed_use = \"w_metro_only;w_bus_metro;d_metro_only;d_bus_metro\"\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        allowed_use = \"w_rail_only;d_rail_only\"\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        allowed_use = \"w_bus_only;w_bus_metro;d_bus_only;d_bus_metro\"\n",
        "    return allowed_use\n",
        "\n",
        "\n",
        "def _allowed_use_transferring(node_type_1, node_type_2):\n",
        "    if (node_type_1 == 'stop') & (node_type_2 == 'stop'):\n",
        "        allowed_use = \"w_bus_only;d_bus_only\"\n",
        "    elif (node_type_1 == 'stop') & (node_type_2 == 'metro_station'):\n",
        "        allowed_use = \"w_bus_metro;d_bus_metro\"\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'stop'):\n",
        "        allowed_use = \"w_bus_metro;d_bus_metro\"\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'metro_station'):\n",
        "        allowed_use = \"w_metro_only;d_metro_only\"\n",
        "    elif (node_type_1 == 'rail_station') & (node_type_2 == 'rail_station'):\n",
        "        allowed_use = \"w_rail_only;d_rail_only\"\n",
        "    else:\n",
        "        allowed_use = \"closed\"\n",
        "\n",
        "    return allowed_use\n",
        "\n",
        "\n",
        "def _transferring_penalty(node_type_1, node_type_2):\n",
        "    if (node_type_1 == 'stop') & (node_type_2 == 'stop'):\n",
        "        VDF_penalty1 = 99\n",
        "    elif (node_type_1 == 'stop') & (node_type_2 == 'metro_station'):\n",
        "        VDF_penalty1 = 0\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'stop'):\n",
        "        VDF_penalty1 = 0\n",
        "    elif (node_type_1 == 'metro_station') & (node_type_2 == 'metro_station'):\n",
        "        VDF_penalty1 = 99\n",
        "    elif (node_type_1 == 'rail_station') & (node_type_2 == 'rail_station'):\n",
        "        VDF_penalty1 = 99\n",
        "    else:\n",
        "        VDF_penalty1 = 1000\n",
        "\n",
        "    return VDF_penalty1\n",
        "\n",
        "\n",
        "def _convert_route_type_to_node_type_p(route_type):\n",
        "    #  convert route type to node type on physical network\n",
        "    node_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        node_type = 'stop'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        node_type = 'metro_station'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        node_type = 'rail_station'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        node_type = 'stop'\n",
        "    return node_type\n",
        "\n",
        "\n",
        "def _convert_route_type_to_node_type_s(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    node_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        node_type = 'tram_service_node'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        node_type = 'metro_service_node'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        node_type = 'rail_service_node'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        node_type = 'bus_service_node'\n",
        "    return node_type\n",
        "\n",
        "\n",
        "def _convert_route_type_to_link_type(route_type):\n",
        "    #  convert route type to node type on service network\n",
        "    link_type = \"\"\n",
        "    if int(route_type) == 0:\n",
        "        # tram\n",
        "        link_type = 'tram'\n",
        "    if int(route_type) == 1:\n",
        "        # metro\n",
        "        link_type = 'metro'\n",
        "    if int(route_type) == 2:\n",
        "        # rail\n",
        "        link_type = 'rail'\n",
        "    if int(route_type) == 3:\n",
        "        # bus\n",
        "        link_type = 'bus'\n",
        "    return link_type\n",
        "\n",
        "\n",
        "def _split_ignore_separators_in_quoted(s, separator=',', quote_mark='\"'):\n",
        "    result = []\n",
        "    quoted = False\n",
        "    current = ''\n",
        "    for i in range(len(s)):\n",
        "        if quoted:\n",
        "            current += s[i]\n",
        "            if s[i] == quote_mark:\n",
        "                quoted = False\n",
        "            continue\n",
        "        if s[i] == separator:\n",
        "            result.append(current.strip())\n",
        "            current = ''\n",
        "        else:\n",
        "            current += s[i]\n",
        "            if s[i] == quote_mark:\n",
        "                quoted = True\n",
        "    result.append(current)\n",
        "    return result\n",
        "\n",
        "\n",
        "def _calculate_distance_from_geometry(lon1, lat1, lon2, lat2):  # WGS84 transfer coordinate system to distance(mile) #xy\n",
        "    radius = 6371\n",
        "    d_latitude = (lat2 - lat1) * math.pi / 180.0\n",
        "    d_longitude = (lon2 - lon1) * math.pi / 180.0\n",
        "\n",
        "    a = math.sin(d_latitude / 2) * math.sin(d_latitude / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(\n",
        "        lat2 * math.pi / 180.0) * math.sin(d_longitude / 2) * math.sin(d_longitude / 2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "    # distance = radius * c * 1000 / 1609.34  # mile\n",
        "    distance = radius * c * 1000  # meter\n",
        "    return distance\n",
        "\n",
        "\n",
        "def _hhmm_to_minutes(time_period_1):\n",
        "    from_time_1 = datetime.time(int(time_period_1[0:2]), int(time_period_1[2:4]))\n",
        "    to_time_1 = datetime.time(int(time_period_1[-4:-2]), int(time_period_1[-2:]))\n",
        "    from_time_min_1 = from_time_1.hour * 60 + from_time_1.minute\n",
        "    to_time_min_1 = to_time_1.hour * 60 + to_time_1.minute\n",
        "    return from_time_min_1, to_time_min_1"
      ],
      "metadata": {
        "id": "TfHTwVuXhVHf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrchDA-R0WN0"
      },
      "source": [
        "**Step 2: Read GTFS data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UL0MUnaHAD5"
      },
      "source": [
        "def reading_data(gtfs_path):\n",
        "    print('start reading input GTFS files...')\n",
        "    agency_df = _reading_text(gtfs_path + os.sep + 'agency')\n",
        "    agency_name = agency_df['agency_name'][0]\n",
        "    if '\"' in agency_name:\n",
        "        agency_name = eval(agency_name)\n",
        "        #  Remove quotes from string, for example '\"Arlington Transit\"' --> 'Arlington Transit'\n",
        "    print(\"agent_name:\", agency_name)\n",
        "\n",
        "    stop_df = _reading_text(gtfs_path + os.sep + 'stops')\n",
        "    stop_df = stop_df[['stop_id', 'stop_name', 'stop_lat', 'stop_lon']]\n",
        "    print(\"number of stops =\", len(stop_df))\n",
        "    #  select only latitude and longitude of the stops/stations\n",
        "\n",
        "    route_df = _reading_text(gtfs_path + os.sep + 'routes')\n",
        "    route_df = route_df[['route_id', 'route_short_name', 'route_long_name', 'route_type']]\n",
        "    print(\"number of routes =\", len(route_df))\n",
        "\n",
        "    trip_df = _reading_text(gtfs_path + os.sep + 'trips')\n",
        "    if 'direction_id' not in trip_df.columns.tolist():  # direction_id is mandatory field name here\n",
        "        trip_df['direction_id'] = str(0)\n",
        "    trip_df['direction_id'] = trip_df.apply(lambda x: str(2 - int(x['direction_id'])), axis=1)\n",
        "    directed_route_id = trip_df['route_id'].astype(str).str.cat(\n",
        "        trip_df['direction_id'].astype(str), sep='.')\n",
        "    trip_df['directed_route_id'] = directed_route_id  # add a new column \"directed_route_id\"\n",
        "    #  If trips on a route service opposite directions,distinguish directions using values 0 and 1.\n",
        "    # revise the direction_id from 0,1 to 2,1\n",
        "    # add a new field directed_route_id\n",
        "    # deal with special issues of Agency 12 Fairfax CUE # Alicia, Nov 10:\n",
        "    # route file has route id with quotes, e.g., '\"green2\"' while trip file does not have it, e.g.,'green2'\n",
        "    if (route_df['route_id'][0][0] == '\"') != (trip_df['route_id'][0][0] == '\"'):\n",
        "        if route_df['route_id'][0][0] == '\"':\n",
        "            route_df['route_id'] = route_df.apply(lambda x: x['route_id'].strip('\"'), axis=1)\n",
        "        else:\n",
        "            trip_df['route_id'] = trip_df.apply(lambda x: x['route_id'].strip('\"'), axis=1)\n",
        "    trip_route_df = pd.merge(trip_df, route_df, on='route_id')  # might go wrong in Agency 12\n",
        "    print(\"number of trips =\", len(trip_route_df), \"...\", len(trip_df))\n",
        "    #  as route is higher level planning than trips, len(trip_route_df)=len(trip_df)\n",
        "\n",
        "    stop_time_df = _reading_text(gtfs_path + os.sep + 'stop_times')\n",
        "    print(\"number of stop_time records =\", len(stop_time_df))\n",
        "    # drop the stations without accurate arrival and departure time.\n",
        "\n",
        "    # drop nan\n",
        "    stop_time_df = stop_time_df.dropna(subset=['arrival_time'], how='any')\n",
        "    # drop ''\n",
        "    stop_time_df = stop_time_df[stop_time_df.arrival_time != '']\n",
        "    stop_time_df = stop_time_df[stop_time_df.departure_time != '']\n",
        "\n",
        "    # drop ' '\n",
        "    stop_time_df = stop_time_df[stop_time_df.arrival_time != ' ']\n",
        "    stop_time_df = stop_time_df[stop_time_df.departure_time != ' ']\n",
        "    print(\"number of stop_time records after dropping empty arrival and departure time =\", len(stop_time_df))\n",
        "\n",
        "    # convert timestamp to minute\n",
        "    # as some agencies might have trips overlapping two days, should use _to_timedelta to convert the data\n",
        "    print(\"start converting the time stamps...\")\n",
        "    tt = datetime.datetime(2021, 1, 1, 0, 0, 0, 0)\n",
        "    stop_time_df['arrival_time'] = pd.to_timedelta(stop_time_df['arrival_time']) + tt\n",
        "    stop_time_df['departure_time'] = pd.to_timedelta(stop_time_df['departure_time']) + tt\n",
        "    stop_time_df['arrival_time'] = \\\n",
        "        stop_time_df['arrival_time'].apply(lambda x: x.hour * 60 + x.minute + 1440 * (x.day - 1))\n",
        "    stop_time_df['departure_time'] = \\\n",
        "        stop_time_df['departure_time'].apply(lambda x: x.hour * 60 + x.minute + 1440 * (x.day - 1))\n",
        "\n",
        "    print(\"start marking terminal flags for stops...\")\n",
        "    iteration_group = stop_time_df.groupby(['trip_id'])\n",
        "    # mark terminal flag for each stop. The terminals can only be determined at the level of trips\n",
        "    input_list = []\n",
        "    time_start = time.time()\n",
        "    for trip_id, trip_stop_time_df in iteration_group:\n",
        "        trip_stop_time_df = trip_stop_time_df.sort_values(by=['stop_sequence'])\n",
        "        trip_stop_time_df = trip_stop_time_df.reset_index()\n",
        "        # select only the trips within the provided time window\n",
        "        if (trip_stop_time_df.arrival_time.min() <= period_end_time) & (\n",
        "                trip_stop_time_df.arrival_time.min() >= period_start_time):\n",
        "            input_list.append(trip_stop_time_df)\n",
        "    intermediate_output_list = list(map(_determine_terminal_flag, input_list))\n",
        "    output_list = list(map(_stop_sequence_label, intermediate_output_list))\n",
        "\n",
        "    # use map function to speed up marking process\n",
        "    time_end = time.time()\n",
        "    print('add terminal_flag for trips using CPU time:', time_end - time_start, 's')\n",
        "\n",
        "    time_start = time.time()\n",
        "    stop_time_df_with_terminal = pd.concat(output_list, axis=0)\n",
        "    # concatenating a list is much faster than concatenating separate dataframes\n",
        "    time_end = time.time()\n",
        "    print('concatenate different trips using CPU time:', time_end - time_start, 's')\n",
        "    print(\"have updated\", len(stop_time_df_with_terminal), \"stop_time records\")\n",
        "    print(\"merge the route information with trip information...\")\n",
        "    directed_trip_route_stop_time_df = pd.merge(trip_route_df, stop_time_df_with_terminal, on='trip_id')\n",
        "    print(\"number of final merged records =\", len(directed_trip_route_stop_time_df))\n",
        "    print(\"Data reading done..\")\n",
        "\n",
        "    #  as trip is higher level planning than stop time scheduling, len(stop_time_df)>=len(trip_df)\n",
        "    #  Each record of directed_trip_route_stop_time_df represents a space-time state of a vehicle\n",
        "    # trip_id (different vehicles, e.g., train lines)\n",
        "    # stop_id (spatial location of the vehicle)\n",
        "    # arrival_time,departure_time (time index of the vehicle)\n",
        "\n",
        "    directed_route_stop_id = directed_trip_route_stop_time_df['directed_route_id'].astype(\n",
        "        str).str.cat(directed_trip_route_stop_time_df['stop_id'].astype(str), sep='.')\n",
        "    directed_trip_route_stop_time_df['directed_route_stop_id'] = directed_route_stop_id\n",
        "    #  directed_route_stop_id is a unique id to identify the route, direction, and stop of a vehicle at a time point\n",
        "    directed_trip_route_stop_time_df['stop_sequence'] \\\n",
        "        = directed_trip_route_stop_time_df['stop_sequence'].astype('int32')\n",
        "    # two important concepts : 1 directed_service_stop_id (directed_route_stop_id + stop sequence)\n",
        "    directed_trip_route_stop_time_df['directed_service_stop_id'] = \\\n",
        "        directed_trip_route_stop_time_df.directed_route_stop_id.astype(str) + ':' + \\\n",
        "        directed_trip_route_stop_time_df.stop_sequence_label\n",
        "    # 2. directed service id (directed_route_id + stop sequence) same directed route id might have different sequences\n",
        "    directed_trip_route_stop_time_df['directed_service_id'] = \\\n",
        "        directed_trip_route_stop_time_df.directed_route_id.astype(str) + ':' + \\\n",
        "        directed_trip_route_stop_time_df.stop_sequence_label\n",
        "    #  attach stop name and geometry for stops\n",
        "    directed_trip_route_stop_time_df = pd.merge(directed_trip_route_stop_time_df, stop_df, on='stop_id')\n",
        "    directed_trip_route_stop_time_df['agency_name'] = agency_name\n",
        "\n",
        "    return stop_df, route_df, trip_df, trip_route_df, stop_time_df, directed_trip_route_stop_time_df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SENYsUzC03bx"
      },
      "source": [
        "**Step 3: Create nodes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5_AiNwZK8wz"
      },
      "source": [
        "def create_nodes(directed_trip_route_stop_time_df, agency_num):\n",
        "    \"\"\"create physical (station) node...\"\"\"\n",
        "    physical_node_df = pd.DataFrame()\n",
        "    temp_df = directed_trip_route_stop_time_df.drop_duplicates(subset=['stop_id'])\n",
        "    physical_node_df['name'] = temp_df['stop_id']\n",
        "    physical_node_df = physical_node_df.sort_values(by=['name'])\n",
        "    physical_node_df['node_id'] = \\\n",
        "        np.linspace(start=1, stop=len(physical_node_df), num=len(physical_node_df)).astype('int32')\n",
        "    physical_node_df['node_id'] += int('{}000000'.format(agency_num))\n",
        "    physical_node_df['physical_node_id'] = physical_node_df['node_id']\n",
        "    physical_node_df['x_coord'] = temp_df['stop_lon'].astype(float)\n",
        "    physical_node_df['y_coord'] = temp_df['stop_lat'].astype(float)\n",
        "    physical_node_df['route_type'] = temp_df['route_type']\n",
        "    physical_node_df['route_id'] = temp_df['route_id']\n",
        "    physical_node_df['node_type'] = \\\n",
        "        physical_node_df.apply(lambda x: _convert_route_type_to_node_type_p(x.route_type), axis=1)\n",
        "    physical_node_df['directed_route_id'] = \"\"\n",
        "    physical_node_df['directed_service_id'] = \"\"\n",
        "    physical_node_df['zone_id'] = \"\"\n",
        "    physical_node_df['agency_name'] = temp_df['agency_name']\n",
        "    physical_node_df['geometry'] = 'POINT (' + physical_node_df['x_coord'].astype(str) + \\\n",
        "                                   ' ' + physical_node_df['y_coord'].astype(str) + ')'\n",
        "    stop_name_id_dict = dict(zip(physical_node_df['name'], physical_node_df['node_id']))\n",
        "    physical_node_df['terminal_flag'] = temp_df['terminal_flag']\n",
        "    physical_node_df['ctrl_type'] = \"\"\n",
        "    physical_node_df['agent_type'] = \"\"\n",
        "\n",
        "    \"\"\" create service node...\"\"\"\n",
        "    service_node_df = pd.DataFrame()\n",
        "    temp_df = directed_trip_route_stop_time_df.drop_duplicates(subset=['directed_service_stop_id'])\n",
        "    # 2.2.2 route stop node\n",
        "    service_node_df['name'] = temp_df['directed_service_stop_id']\n",
        "    service_node_df = service_node_df.sort_values(by=['name'])\n",
        "    service_node_df['node_id'] = \\\n",
        "        np.linspace(start=1, stop=len(service_node_df), num=len(service_node_df)).astype('int32')\n",
        "    service_node_df['physical_node_id'] = temp_df.apply(lambda x: stop_name_id_dict[x.stop_id], axis=1)\n",
        "    service_node_df['node_id'] += int('{}500000'.format(agency_num))\n",
        "\n",
        "    service_node_df['x_coord'] = temp_df['stop_lon'].astype(float) - 0.000100\n",
        "    service_node_df['y_coord'] = temp_df['stop_lat'].astype(float) - 0.000100\n",
        "    service_node_df['route_type'] = temp_df['route_type']\n",
        "    service_node_df['route_id'] = temp_df['route_id']\n",
        "    service_node_df['node_type'] = \\\n",
        "        service_node_df.apply(lambda x: _convert_route_type_to_node_type_s(x.route_type), axis=1)\n",
        "    # node_csv['terminal_flag'] = ' '\n",
        "    service_node_df['directed_route_id'] = temp_df['directed_route_id'].astype(str)\n",
        "    service_node_df['directed_service_id'] = temp_df['directed_service_id'].astype(str)\n",
        "    service_node_df['zone_id'] = \"\"\n",
        "    service_node_df['agency_name'] = temp_df['agency_name']\n",
        "    service_node_df['geometry'] = \\\n",
        "        'POINT (' + service_node_df['x_coord'].astype(str) + ' ' + service_node_df['y_coord'].astype(str) + ')'\n",
        "\n",
        "    service_node_df['terminal_flag'] = temp_df['terminal_flag']\n",
        "    service_node_df['ctrl_type'] = \"\"\n",
        "    service_node_df['agent_type'] = \"\"\n",
        "    # concatenate service and physical node\n",
        "    node_df = pd.concat([physical_node_df, service_node_df])\n",
        "    return node_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfLuLzVs4SJS"
      },
      "source": [
        "**Step 4: Create service boarding links**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_service_boarding_links(directed_trip_route_stop_time_df, node_df, agency_num, one_agency_link_list):\n",
        "    \"\"\"dictionaries\"\"\"\n",
        "    node_id_dict = dict(zip(node_df['name'], node_df['node_id']))\n",
        "    directed_service_dict = dict(zip(node_df['node_id'], node_df['name']))\n",
        "    node_lon_dict = dict(zip(node_df['node_id'], node_df['x_coord']))\n",
        "    node_lat_dict = dict(zip(node_df['node_id'], node_df['y_coord']))\n",
        "    frequency_dict = {}\n",
        "\n",
        "    print(\"1. start creating route links...\")\n",
        "    \"\"\"service links\"\"\"\n",
        "    number_of_route_links = 0\n",
        "    iteration_group = directed_trip_route_stop_time_df.groupby('directed_service_id')\n",
        "    labeled_directed_service_list = []\n",
        "\n",
        "    time_start = time.time()\n",
        "    for directed_service_id, route_df in iteration_group:\n",
        "        if directed_service_id in labeled_directed_service_list:\n",
        "            continue\n",
        "        else:\n",
        "            labeled_directed_service_list.append(directed_service_id)\n",
        "            number_of_trips = len(route_df.trip_id.unique())\n",
        "            frequency_dict[directed_service_id] = number_of_trips  # note the frequency of routes\n",
        "            one_line_df = route_df[route_df.trip_id == route_df.trip_id.unique()[0]]\n",
        "            one_line_df = one_line_df.sort_values(by=['stop_sequence'])\n",
        "            number_of_records = len(one_line_df)\n",
        "            one_line_df = one_line_df.reset_index()\n",
        "            for k in range(number_of_records - 1):\n",
        "                link_id = 1000000 * agency_num + number_of_route_links + 1\n",
        "                from_node_id = node_id_dict[one_line_df.iloc[k].directed_service_stop_id]\n",
        "                to_node_id = node_id_dict[one_line_df.iloc[k + 1].directed_service_stop_id]\n",
        "                facility_type = _convert_route_type_to_link_type(one_line_df.iloc[k].route_type)\n",
        "                dir_flag = 1\n",
        "                directed_route_id = one_line_df.iloc[k].directed_route_id\n",
        "                link_type = 1\n",
        "                link_type_name = 'service_links'\n",
        "                from_node_lon = float(one_line_df.iloc[k].stop_lon)\n",
        "                from_node_lat = float(one_line_df.iloc[k].stop_lat)\n",
        "                to_node_lon = float(one_line_df.iloc[k + 1].stop_lon)\n",
        "                to_node_lat = float(one_line_df.iloc[k + 1].stop_lat)\n",
        "                length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "                lanes = number_of_trips\n",
        "                capacity = 999999\n",
        "                VDF_fftt1 = one_line_df.iloc[k + 1].arrival_time - one_line_df.iloc[k].arrival_time\n",
        "                # minutes\n",
        "                VDF_cap1 = lanes * capacity\n",
        "                free_speed = ((length / 1000) / (VDF_fftt1 + 0.001)) * 60\n",
        "                # (kilometers/minutes)*60 = kilometer/hour\n",
        "                VDF_alpha1 = 0.15\n",
        "                VDF_beta1 = 4\n",
        "                VDF_penalty1 = 0\n",
        "                cost = 0\n",
        "                geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                           str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "                agency_name = one_line_df.agency_name[0]\n",
        "                allowed_use = _allowed_use_function(one_line_df.iloc[k].route_type)\n",
        "                stop_sequence = one_line_df.iloc[k].stop_sequence\n",
        "                directed_service_id = one_line_df.iloc[k].directed_service_id\n",
        "                link_list = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                             link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                             VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                             agency_name,\n",
        "                             stop_sequence, directed_service_id]\n",
        "                one_agency_link_list.append(link_list)\n",
        "                number_of_route_links += 1\n",
        "                if number_of_route_links % 50 == 0:\n",
        "                    time_end = time.time()\n",
        "                    print('convert ', number_of_route_links,\n",
        "                          'service links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    print(\"2. start creating boarding links from stations to their passing routes...\")\n",
        "    \"\"\"boarding_links\"\"\"\n",
        "    service_node_df = node_df[node_df.node_id != node_df.physical_node_id]\n",
        "    #  select service node from node_df\n",
        "    service_node_df = service_node_df.reset_index()\n",
        "    number_of_sta2route_links = 0\n",
        "    for iter, row in service_node_df.iterrows():\n",
        "        link_id = agency_num * 1000000 + number_of_route_links + number_of_sta2route_links\n",
        "        from_node_id = row.physical_node_id\n",
        "        to_node_id = row.node_id\n",
        "        facility_type = _convert_route_type_to_link_type(row.route_type)\n",
        "        dir_flag = 1\n",
        "        directed_route_id = row.directed_route_id\n",
        "        link_type = 2\n",
        "        link_type_name = 'boarding_links'\n",
        "        to_node_lon = row.x_coord\n",
        "        to_node_lat = row.y_coord\n",
        "        from_node_lon = node_lon_dict[row.physical_node_id]\n",
        "        from_node_lat = node_lat_dict[row.physical_node_id]\n",
        "        length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "        free_speed = 2\n",
        "        lanes = 1\n",
        "        capacity = 999999\n",
        "        VDF_cap1 = lanes * capacity\n",
        "        VDF_alpha1 = 0.15\n",
        "        VDF_beta1 = 4\n",
        "        VDF_penalty1 = 0\n",
        "        cost = 0\n",
        "        stop_sequence = -1\n",
        "        directed_service_id = directed_service_dict[to_node_id]\n",
        "        geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                   str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "        agency_name = row.agency_name\n",
        "        allowed_use = _allowed_use_function(row.route_type)\n",
        "\n",
        "        # inbound links (boarding)\n",
        "\n",
        "        VDF_fftt1 = \\\n",
        "            0.5 * ((period_end_time - period_start_time) / frequency_dict[row.directed_service_id])\n",
        "        VDF_fftt1 = min(VDF_fftt1, 10)\n",
        "        # waiting time at a station is 10 minutes at most\n",
        "        geometry = 'LINESTRING (' + str(to_node_lon) + ' ' + str(to_node_lat) + ', ' + \\\n",
        "                   str(from_node_lon) + ' ' + str(from_node_lat) + ')'\n",
        "        # inbound link is average waiting time derived from frequency\n",
        "        link_list_inbound = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                             link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                             VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                             agency_name,\n",
        "                             stop_sequence, directed_service_id]\n",
        "        number_of_sta2route_links += 1\n",
        "\n",
        "        # outbound links (boarding)\n",
        "        link_id = agency_num * 1000000 + number_of_route_links + number_of_sta2route_links\n",
        "        VDF_fftt1 = 1  # (length / free_speed) * 60\n",
        "        #  the time of outbound time\n",
        "        link_list_outbound = [link_id, to_node_id, from_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                              link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                              VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use,\n",
        "                              agency_name,\n",
        "                              stop_sequence, directed_service_id]\n",
        "        one_agency_link_list.append(link_list_inbound)\n",
        "        one_agency_link_list.append(link_list_outbound)\n",
        "        number_of_sta2route_links += 1\n",
        "        #  one inbound link and one outbound link\n",
        "        if number_of_sta2route_links % 50 == 0:\n",
        "            time_end = time.time()\n",
        "            print('convert ', number_of_sta2route_links,\n",
        "                  'boarding links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    return one_agency_link_list"
      ],
      "metadata": {
        "id": "ee8UoBmig-6j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Create transfering links**"
      ],
      "metadata": {
        "id": "eKfmU1vghGRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transferring_links(all_node_df, all_link_list):\n",
        "    physical_node_df = all_node_df[all_node_df.node_id == all_node_df.physical_node_id]\n",
        "    physical_node_df = physical_node_df.reset_index()\n",
        "    number_of_transferring_links = 0\n",
        "    time_start = time.time()\n",
        "    for i in range(len(physical_node_df)):\n",
        "        ref_x = physical_node_df.iloc[i].x_coord\n",
        "        ref_y = physical_node_df.iloc[i].y_coord\n",
        "        neighboring_node_df = physical_node_df[(physical_node_df.x_coord >= (ref_x - 0.003)) &\n",
        "                                               (physical_node_df.x_coord <= (ref_x + 0.003))]\n",
        "        neighboring_node_df = neighboring_node_df[(neighboring_node_df.y_coord >= (ref_y - 0.003)) &\n",
        "                                                  (neighboring_node_df.y_coord <= (ref_y + 0.003))]\n",
        "        labeled_list = []\n",
        "        count = 0\n",
        "        for j in range(len(neighboring_node_df)):\n",
        "            if count >= 10:\n",
        "                break\n",
        "            if (physical_node_df.iloc[i].route_id, physical_node_df.iloc[i].agency_name) == \\\n",
        "                    (neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name):\n",
        "                continue\n",
        "            from_node_lon = float(physical_node_df.iloc[i].x_coord)\n",
        "            from_node_lat = float(physical_node_df.iloc[i].y_coord)\n",
        "            to_node_lon = float(neighboring_node_df.iloc[j].x_coord)\n",
        "            to_node_lat = float(neighboring_node_df.iloc[j].y_coord)\n",
        "            length = _calculate_distance_from_geometry(from_node_lon, from_node_lat, to_node_lon, to_node_lat)\n",
        "            if (length > 321.869) | (length < 1):\n",
        "                continue\n",
        "            if (neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name) in labeled_list:\n",
        "                continue\n",
        "            count += 1\n",
        "            labeled_list.append((neighboring_node_df.iloc[j].route_id, neighboring_node_df.iloc[j].agency_name))\n",
        "            # consider only one stops of another route\n",
        "            # transferring 1\n",
        "            #  print('transferring link length =', length)\n",
        "            link_id = number_of_transferring_links + 1\n",
        "            from_node_id = physical_node_df.iloc[i].node_id\n",
        "            to_node_id = neighboring_node_df.iloc[j].node_id\n",
        "            facility_type = 'sta2sta'\n",
        "            dir_flag = 1\n",
        "            directed_route_id = -1\n",
        "            link_type = 3\n",
        "            link_type_name = 'transferring_links'\n",
        "            lanes = 1\n",
        "            capacity = 999999\n",
        "            VDF_fftt1 = (length / 1000) / 1\n",
        "            VDF_cap1 = lanes * capacity\n",
        "            free_speed = 1\n",
        "            # 1 kilo/hour\n",
        "            VDF_alpha1 = 0.15\n",
        "            VDF_beta1 = 4\n",
        "            VDF_penalty1 = _transferring_penalty(physical_node_df.iloc[i].node_type, neighboring_node_df.iloc[j].node_type)\n",
        "            # penalty of transferring\n",
        "            cost = 60\n",
        "            geometry = 'LINESTRING (' + str(from_node_lon) + ' ' + str(from_node_lat) + ', ' + \\\n",
        "                       str(to_node_lon) + ' ' + str(to_node_lat) + ')'\n",
        "            agency_name = \"\"\n",
        "            allowed_use = \\\n",
        "                _allowed_use_transferring(physical_node_df.iloc[i].node_type, neighboring_node_df.iloc[j].node_type)\n",
        "            stop_sequence = \"\"\n",
        "            directed_service_id = \"\"\n",
        "            link_list = [link_id, from_node_id, to_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                         link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                         VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use, agency_name,\n",
        "                         stop_sequence, directed_service_id]\n",
        "            all_link_list.append(link_list)\n",
        "            # transferring 2\n",
        "            number_of_transferring_links += 1\n",
        "            geometry = 'LINESTRING (' + str(to_node_lon) + ' ' + str(to_node_lat) + ', ' + \\\n",
        "                       str(from_node_lon) + ' ' + str(from_node_lat) + ')'\n",
        "            link_id = number_of_transferring_links + 1\n",
        "            link_list = [link_id, to_node_id, from_node_id, facility_type, dir_flag, directed_route_id,\n",
        "                         link_type, link_type_name, length, lanes, capacity, free_speed, cost,\n",
        "                         VDF_fftt1, VDF_cap1, VDF_alpha1, VDF_beta1, VDF_penalty1, geometry, allowed_use, agency_name,\n",
        "                         stop_sequence, directed_service_id]\n",
        "            all_link_list.append(link_list)\n",
        "            number_of_transferring_links += 1\n",
        "            if number_of_transferring_links % 50 == 0:\n",
        "                time_end = time.time()\n",
        "                print('convert ', number_of_transferring_links,\n",
        "                      'transferring links successfully...', 'using time', time_end - time_start, 's')\n",
        "\n",
        "    return all_link_list"
      ],
      "metadata": {
        "id": "nMPHC6f6hMGv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: gtfs2gmns Main Function**"
      ],
      "metadata": {
        "id": "AW9zUzoXh3Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gtfs2gmns(input_path, output_path):\n",
        "    start_time = time.time()\n",
        "    folders = os.listdir(input_path)\n",
        "    gtfs_folder_list = []\n",
        "    for sub_folder in folders:\n",
        "        sub_folder_path = input_path + '/' + sub_folder\n",
        "        if os.path.isdir(sub_folder_path):  # check whether the specified path is an existing directory or not.\n",
        "            gtfs_folder_list.append(sub_folder_path)\n",
        "    if len(gtfs_folder_list) == 0:\n",
        "        gtfs_folder_list.append(input_path)\n",
        "\n",
        "    all_node_list = []\n",
        "    all_link_list = []\n",
        "    for i in range(len(gtfs_folder_list)):\n",
        "        print('Start converting Agency_{}...'.format(i + 1))\n",
        "        print('Directory : ' + str(gtfs_folder_list[i]))\n",
        "        agency_gtfs_path = gtfs_folder_list[i]\n",
        "        \"\"\" step 1. reading data \"\"\"\n",
        "        stop_df, route_df, trip_df, trip_route_df, stop_time_df, directed_trip_route_stop_time_df = \\\n",
        "            reading_data(agency_gtfs_path)\n",
        "        #  directed_trip_route_stop_time_df.to_csv(gtfs_folder_list[i] + '/timetable.csv', index=False)\n",
        "        #  directed_trip_route_stop_time_df = pd.read_csv(gtfs_folder_list[i] + '/timetable.csv')\n",
        "\n",
        "        \"\"\"step 2. create nodes\"\"\"\n",
        "        agency_num = i + 1\n",
        "        # number of agency equals to i+1\n",
        "        node_df = create_nodes(directed_trip_route_stop_time_df, agency_num)\n",
        "        all_node_list.append(node_df)\n",
        "        print(\"node.csv of\", str(gtfs_folder_list[i]), \"has been generated...\")\n",
        "        \"\"\"step 3. create links\"\"\"\n",
        "        all_link_list \\\n",
        "            = create_service_boarding_links(directed_trip_route_stop_time_df, node_df, agency_num, all_link_list)\n",
        "\n",
        "        if i == len(gtfs_folder_list):\n",
        "            print('output')\n",
        "        print('Conversion of  Agency{}...'.format(agency_num + 1), ' have done..')\n",
        "\n",
        "    all_node_df = pd.concat(all_node_list)\n",
        "    all_node_df.reset_index(inplace=True)\n",
        "    all_node_df = all_node_df.drop(['index'], axis=1)\n",
        "    # transferring links\n",
        "    all_link_list = create_transferring_links(all_node_df, all_link_list)\n",
        "\n",
        "    all_link_df = pd.DataFrame(all_link_list)\n",
        "    all_link_df.rename(columns={0: 'link_id',\n",
        "                                1: 'from_node_id',\n",
        "                                2: 'to_node_id',\n",
        "                                3: 'facility_type',\n",
        "                                4: 'dir_flag',\n",
        "                                5: 'directed_route_id',\n",
        "                                6: 'link_type',\n",
        "                                7: 'link_type_name',\n",
        "                                8: 'length',\n",
        "                                9: 'lanes',\n",
        "                                10: 'capacity',\n",
        "                                11: 'free_speed',\n",
        "                                12: 'cost',\n",
        "                                13: 'VDF_fftt1',\n",
        "                                14: 'VDF_cap1',\n",
        "                                15: 'VDF_alpha1',\n",
        "                                16: 'VDF_beta1',\n",
        "                                17: 'VDF_penalty1',\n",
        "                                18: 'geometry',\n",
        "                                19: 'VDF_allowed_uses1',\n",
        "                                20: 'agency_name',\n",
        "                                21: 'stop_sequence',\n",
        "                                22: 'directed_service_id'}, inplace=True)\n",
        "    all_node_df.to_csv('node.csv', index=False)\n",
        "    #  zone_df = pd.read_csv('zone.csv')\n",
        "    #  source_node_df = pd.read_csv('source_node.csv')\n",
        "    #  node_df = pd.concat([zone_df, all_node_df])\n",
        "    #  node_df.to_csv(\"node.csv\", index=False)\n",
        "    all_link_df = all_link_df.drop_duplicates(\n",
        "        subset=['from_node_id', 'to_node_id'],\n",
        "        keep='last').reset_index(drop=True)\n",
        "    all_link_df.to_csv('link.csv', index=False)\n",
        "    print('run time -->', time.time() - start_time)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    global period_start_time\n",
        "    global period_end_time\n",
        "    time_period_id = 1\n",
        "    time_period = '1200_1300'\n",
        "    period_start_time, period_end_time = _hhmm_to_minutes(time_period)"
      ],
      "metadata": {
        "id": "LHYAXQYEh2bC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Download GMNS data files**"
      ],
      "metadata": {
        "id": "0mw8Jzv2ikXI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRYOkJyT4juC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb21913-53ec-46f8-b8d2-d53880d0ab27"
      },
      "source": [
        "% cd ../\n",
        "!zip -r /content/GTFS2GMNS/gmns.zip /content/GTFS2GMNS/\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GTFS2GMNS/test\n",
            "  adding: content/GTFS2GMNS/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/description (deflated 14%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/refs/heads/main (deflated 25%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/refs/remotes/origin/HEAD (deflated 25%)\n",
            "  adding: content/GTFS2GMNS/.git/logs/HEAD (deflated 25%)\n",
            "  adding: content/GTFS2GMNS/.git/objects/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/objects/pack/pack-a8e4088c3342f15e4fc4235efefdd63cd6bdd110.pack (deflated 0%)\n",
            "  adding: content/GTFS2GMNS/.git/objects/pack/pack-a8e4088c3342f15e4fc4235efefdd63cd6bdd110.idx (deflated 8%)\n",
            "  adding: content/GTFS2GMNS/.git/objects/info/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/pre-push.sample (deflated 50%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/pre-commit.sample (deflated 43%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/GTFS2GMNS/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/GTFS2GMNS/.git/refs/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/refs/heads/main (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/index (deflated 42%)\n",
            "  adding: content/GTFS2GMNS/.git/HEAD (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/info/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/info/exclude (deflated 28%)\n",
            "  adding: content/GTFS2GMNS/.git/packed-refs (deflated 10%)\n",
            "  adding: content/GTFS2GMNS/.git/branches/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/.git/config (deflated 30%)\n",
            "  adding: content/GTFS2GMNS/README.md (deflated 55%)\n",
            "  adding: content/GTFS2GMNS/.gitattributes (deflated 6%)\n",
            "  adding: content/GTFS2GMNS/src/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/src/gtfs2gmns.py (deflated 80%)\n",
            "  adding: content/GTFS2GMNS/dataset/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/Pittsburgh.rar (deflated 1%)\n",
            "  adding: content/GTFS2GMNS/dataset/Washington_DC.zip (stored 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/Phoenix.zip (stored 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/Philadelphia.rar (deflated 1%)\n",
            "  adding: content/GTFS2GMNS/dataset/Raleigh.rar (deflated 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/pic/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/pic/Raleigh.PNG (deflated 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/pic/Pittsburgh .PNG (deflated 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/pic/Phoenix.PNG (deflated 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/pic/Philadelphia.PNG (deflated 0%)\n",
            "  adding: content/GTFS2GMNS/dataset/San Francisco.zip (stored 0%)\n",
            "  adding: content/GTFS2GMNS/gtfs2gmns.ipynb (deflated 82%)\n",
            "  adding: content/GTFS2GMNS/test/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/test/link.csv (deflated 93%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/directions.txt (deflated 76%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/calendar_dates.txt (deflated 92%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/shapes.txt (deflated 84%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/frequencies.txt (deflated 33%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/feed_info.txt (deflated 44%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/definitions.txt (deflated 46%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/routes.txt (deflated 66%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/stops.txt (deflated 75%)\n",
            "  adding: content/GTFS2GMNS/test/GTFS/Phoenix/agency.txt (deflated 29%)\n",
            "  adding: content/GTFS2GMNS/test/node.csv (deflated 94%)\n",
            "  adding: content/GTFS2GMNS/doc/ (stored 0%)\n",
            "  adding: content/GTFS2GMNS/doc/GTFS2GMNS Users' Guide.pdf (deflated 5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDf0i_KS7dIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4eb07ea6-c209-43fb-f8b3-07da74008f57"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/GTFS2GMNS/gmns.zip\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_16cb9e08-26e6-40ab-8a9c-af3c46dd18d1\", \"gmns.zip\", 430897325)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6pyZEum8Ood"
      },
      "source": [
        "**Step 8: Visualization using GMNS tool:**\n",
        "By simply uploading node.csv and link.csv at https://asu-trans-ai-lab.github.io/index.html#,  \n",
        " you can easily create custom online maps for any GMNS network files. \n",
        "To view zone and demand information please visit this page to use QGIS/NeXTA tools. https://github.com/asu-trans-ai-lab/traffic-engineering-and-analysis/blob/master/undergraduate_student_project/QGIS%20For%20Gmns%20User%20Guide_v0.5.pdf "
      ]
    }
  ]
}